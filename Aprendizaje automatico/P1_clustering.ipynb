{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2: Aprendizaje automático\n",
    "\n",
    "__Fecha de entrega: 16 de mayo de 2021__\n",
    "\n",
    "El objetivo de esta práctica es aplicar los distintos algoritmos de aprendizaje automático disponibles en la scikit-learn [sklearn](https://scikit-learn.org/stable/) sobre varios conjuntos de datos y aprender a interpretar los resultados obtenidos. La práctica consta de 3 notebooks que se entregarán simultáneamente en la tarea de entrega habilitada en el Campus  Virtual.\n",
    "\n",
    "Lo más importante en esta práctica no es el código Python, sino el análisis de los datos y modelos que construyas y las explicaciones razonadas de cada una de las decisiones que tomes. __No se valorarán trozos de código o gráficas sin ningún tipo de contexto o explicación__.\n",
    "\n",
    "Finalmente, recuerda establecer el parámetro `random_state` en todas las funciones que tomen decisiones aleatorias para que los resultados sean reproducibles (los resultados no varíen entre ejecuciones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartado 1: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Número de grupo: XX__\n",
    "\n",
    "__Nombres de los estudiantes: XXX y XXX__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Carga del conjunto de datos\n",
    "\n",
    "Crea un dataframe a partir del fichero `countries_of_the_world.csv` que se proporciona junto con la práctica. Usa como índice el nombre de los países. \n",
    "\n",
    "Vamos a eliminar la columna `Region` por ser categórica y todas las filas en las que faltan valores usando la operación `dropna`.\n",
    "\n",
    "Muestra el dataframe resultante y explica cuántos países y variables contiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-7503bc3fc9a2>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7503bc3fc9a2>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    df.drop(['Region'], axis=1)/*axis=0 fila; axis = 1 columna*/\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('countries_of_the_world.csv')\n",
    "df.set_index(\"Nombre\")\n",
    "df.drop(['Region'], axis=1)/*axis=0 fila; axis = 1 columna*/\n",
    "df.dropna()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Análisis de los datos\n",
    "\n",
    "En este notebook vamos a trabajar con un subconjunto de las variables. Crea un nuevo dataframe que sólo contenga las variables `GDP ($ per capita)`, `Literacy (%)`, `Phones (per 1000)`, `Agriculture`, `Industry` y `Service`. ¿Qué crees que representan cada una de esas variables?\n",
    "\n",
    "Analiza razonadamente las distribuciones de cada una de las variables (medias, desviaciones típicas, rangos, ...) y las principales relaciones entre pares de variables (diagrama de dispersión, coeficientes de correlación, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Preprocesar los datos\n",
    "\n",
    "Teniendo en cuenta que vamos a utilizar el algoritmo k-Means para encontrar grupos de países similares, explica razonadamente si es necesario o no cambiar la escalas de los datos y si a priori es mejor reescalarlos (MinMaxScaler) o estandarizarlos (StandarScaler).\n",
    "\n",
    "Si decides preprocesarlos, accede al array interno del dataframe y crea un nuevo array con los datos escalados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tenemos que agrupar en funcion de X, y despues lo utilizamos para los clusters\n",
    "#ya que en nuestro archivo no manejaremos numeros negativos hemos decidido normalizar(MinMaxScaler)\n",
    "datos_escalados = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "datos_escalados = datos_escalados.fit_transform(df)\n",
    "print(\"Datos escalados:\\n\", datos_escalados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# buscado en internet\n",
    "Es mas util normalizar, ya que no manejaremos valores valores negativos, y tenemos un rango de valores muy amplio que nos vendra bien escalar.\n",
    "\n",
    "MinMaxScaler(normalizar segun el profesor) : Es posible que nuestro vector característico incluya valores extremadamente altos y/o extremadamente bajos,\n",
    "por eso es importante escalar esos valores para que nuestro algoritmo opere con valores cercanos, y así evitamos características con valores desorbitantes.\n",
    "\n",
    "StandardScaler: Transforma los datos de tal manera que tiene una media de 0 y una desviación estándar de 1.\n",
    "En resumen, estandariza los datos . La estandarización es útil para datos que tienen valores negativos.\n",
    "Se organiza los datos en una distribución normal estándar . Es más útil en clasificación que en regresión.\n",
    "\n",
    "Normalizer: Exprime los datos entre 0 y 1. Realiza la normalización .\n",
    "Debido a la disminución del rango y la magnitud, los gradientes en el proceso de entrenamiento no explotan y no obtienes valores más altos de pérdida.\n",
    "Es más útil en regresión que en clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Encontrar el número óptimo de clusters\n",
    "\n",
    "Decide razonadamente el número óptimo de clusters en el rango 2..10. Ten en cuenta que para interpretar los datos no nos interesa tampoco tener un número excesivo de clusters. Para hacerlo calcula y pinta el diagrama del codo, el índice davies_boulding y el coeficiente silhouette en función del número de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "\n",
    "K_MAX = 10\n",
    "score = np.zeros(K_MAX-2)\n",
    "davies_boulding = np.zeros(K_MAX-2)\n",
    "silhouette = np.zeros(K_MAX-2)\n",
    "for k in range(2, K_MAX): \n",
    "    km = KMeans(init='random', n_clusters=k, random_state=RANDOM_STATE)\n",
    "    km.fit(X)\n",
    "    plot_clusters(X, km.labels_, km.cluster_centers_)\n",
    "    \n",
    "    score[k-2] = -1 * km.score(X)\n",
    "    davies_boulding[k-2] = davies_bouldin_score(X, km.labels_)\n",
    "    silhouette[k-2] = silhouette_score(X, km.labels_)\n",
    "    \n",
    "#mostraremos los tres diagramas para comparar y comprobar cual es el numero optimo de clusters\n",
    "# diagrama del codo\n",
    "plt.plot(range(2, K_MAX), score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indice davies boulding\n",
    "plt.plot(range(2, K_MAX), davies_boulding)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Davies Boulding value')\n",
    "plt.title('Valor de Davies Boulding para diferentes k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette\n",
    "plt.plot(range(2, K_MAX), silhouette)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette value')\n",
    "plt.title('Valor de Silhouette para diferentes k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Descripción de los clusters\n",
    "\n",
    "Describe los clusters que has obtenido en el apartado anterior y trata identificar el grupo de países que contienen. Si te han salido más de 3 elige 3 de ellos que sean bastante diferentes entre sí. \n",
    "\n",
    "Para hacerlo estudia sus descriptores estadísticos y pinta el diagrama de dispersión en función de cada par de variables usando colores diferentes para cada cluster. ¿Qué clusters se separan mejor y en función de qué variables? ¿y cuáles se confunden más?\n",
    "\n",
    "__Cuidado__: para poder interpretar correctamente los datos necesitas que estén en su escala original. Si decidiste escalar los datos, deberás ejecutar kMeans con los datos escalados pero asignar las etiquetas de clusters al conjunto de datos inicial. En este caso es muy sencillo porque el algoritmo no cambia el orden de los datos así que puedes crear directamente una nueva columna en el dataframe original con esas etiquetas. Puede que aparezca un SettingWithCopyWarning por asignar una nueva columna en un dataframe que es una vista de otro dataframe. Puedes ignorar este aviso o puedes hacer una copia del dataframe con `copy` para que no comparta memoria con el otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
